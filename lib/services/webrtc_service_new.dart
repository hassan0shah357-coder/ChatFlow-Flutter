// lib/services/webrtc_service_new.dart - Complete rewrite for better reliability
import 'package:node_chat/services/socket_service.dart';
import 'package:node_chat/services/audio_routing_service.dart';
import 'package:flutter_webrtc/flutter_webrtc.dart';
import 'package:flutter/foundation.dart';
import 'package:audio_session/audio_session.dart';

class WebRTCServiceNew {
  static RTCVideoRenderer localRenderer = RTCVideoRenderer();
  static RTCVideoRenderer remoteRenderer = RTCVideoRenderer();
  static RTCPeerConnection? peerConnection;
  static MediaStream? localStream;
  static bool isInitialized = false;

  // Callbacks
  static VoidCallback? _onConnectionEstablished;

  // Call state
  static String currentCallType = 'voice';
  static bool isOfferer = false;
  static String? peerId;

  // Pending offer storage (for when offer arrives before call acceptance)
  static Map<String, dynamic>? _pendingOffer;

  // ICE candidate buffering for faster connection
  static List<RTCIceCandidate> _pendingIceCandidates = [];
  static bool _isProcessingOffer = false;

  // ICE servers configuration with faster gathering
  static const Map<String, dynamic> _iceServers = {
    'iceServers': [
      {'urls': 'stun:stun.l.google.com:19302'},
      {'urls': 'stun:stun1.l.google.com:19302'},
      // Add free TURN servers for better connectivity in restrictive networks
      {
        'urls': 'turn:openrelay.metered.ca:80',
        'username': 'openrelayproject',
        'credential': 'openrelayproject',
      },
      {
        'urls': 'turn:openrelay.metered.ca:443',
        'username': 'openrelayproject',
        'credential': 'openrelayproject',
      },
    ],
    'iceCandidatePoolSize': 10,
    'iceTransportPolicy': 'all', // Allow all candidates
    'bundlePolicy': 'max-bundle', // Bundle all media on single connection
    'rtcpMuxPolicy': 'require', // Multiplex RTP and RTCP
  };

  // Media constraints optimized for voice communication - reduced processing to minimize noise
  static Map<String, dynamic> get _audioConstraints => {
    'mandatory': {
      'googEchoCancellation': true,
      'googAutoGainControl': false, // Disable AGC to reduce noise
      'googNoiseSuppression': false, // Disable NS to reduce noise
      'googTypingNoiseDetection': false,
      'googHighpassFilter': false,
      'googAudioMirroring': false,
    },
    'optional': [],
  };

  static Map<String, dynamic> get _videoConstraints => {
    'mandatory': {
      'minWidth': 320,
      'minHeight': 240,
      'maxWidth': 1280,
      'maxHeight': 720,
      'minFrameRate': 15,
      'maxFrameRate': 30,
    },
    'facingMode': 'user',
    'optional': [],
  };

  /// Initialize WebRTC service
  static Future<void> init() async {
    try {
      debugPrint('ğŸ¥ Initializing WebRTC Service');

      if (isInitialized) {
        debugPrint('ğŸ¥ WebRTC already initialized, cleaning up first');
        await dispose();
      }

      // Initialize audio session for communication
      await _initializeAudioSession();

      // Initialize audio routing service
      await AudioRoutingService.instance.initialize();

      // Initialize renderers
      localRenderer = RTCVideoRenderer();
      remoteRenderer = RTCVideoRenderer();

      await localRenderer.initialize();
      await remoteRenderer.initialize();

      isInitialized = true;
      debugPrint('ğŸ¥ WebRTC Service initialized successfully');
    } catch (e) {
      debugPrint('ğŸ¥ Error initializing WebRTC: $e');
      rethrow;
    }
  }

  /// Initialize audio session for communication mode
  static Future<void> _initializeAudioSession() async {
    try {
      debugPrint('ğŸ¥ ğŸ”Š Initializing audio session for communication');

      final session = await AudioSession.instance;

      // Configure for voice communication with earpiece priority
      await session.configure(
        const AudioSessionConfiguration(
          avAudioSessionCategory: AVAudioSessionCategory.playAndRecord,
          avAudioSessionMode: AVAudioSessionMode.voiceChat,
          androidAudioAttributes: AndroidAudioAttributes(
            contentType: AndroidAudioContentType.speech,
            usage: AndroidAudioUsage.voiceCommunication,
          ),
          androidAudioFocusGainType: AndroidAudioFocusGainType.gain,
        ),
      );

      // Activate the audio session
      await session.setActive(true);

      debugPrint('ğŸ¥ ğŸ”Š Audio session configured and activated successfully');
    } catch (e) {
      debugPrint('ğŸ¥ ğŸ”Š Error configuring audio session: $e');
      // Don't rethrow - audio session configuration failure shouldn't prevent WebRTC initialization
    }
  }

  /// Set connection callback
  static void setConnectionCallback(VoidCallback callback) {
    _onConnectionEstablished = callback;
  }

  /// Debug method to check WebRTC state
  static void debugWebRTCState() {
    if (peerConnection != null) {
      debugPrint('ğŸ¥ ğŸ” WebRTC Debug State:');
      debugPrint('ğŸ¥   Connection State: ${peerConnection!.connectionState}');
      debugPrint('ğŸ¥   Signaling State: ${peerConnection!.signalingState}');
      debugPrint(
        'ğŸ¥   ICE Connection State: ${peerConnection!.iceConnectionState}',
      );
      debugPrint(
        'ğŸ¥   ICE Gathering State: ${peerConnection!.iceGatheringState}',
      );
      debugPrint('ğŸ¥   Local Stream: ${localStream != null}');
      debugPrint(
        'ğŸ¥   Remote Renderer has stream: ${remoteRenderer.srcObject != null}',
      );
    } else {
      debugPrint('ğŸ¥ ğŸ” No WebRTC peer connection exists');
    }
  }

  /// Create media stream with proper audio/video setup
  static Future<MediaStream> createMediaStream(String callType) async {
    try {
      currentCallType = callType;
      debugPrint('ğŸ¥ Creating media stream for $callType call');

      final Map<String, dynamic> mediaConstraints = {
        'audio': _audioConstraints,
        'video': callType == 'video' ? _videoConstraints : false,
      };

      debugPrint('ğŸ¥ Media constraints: $mediaConstraints');

      final stream = await navigator.mediaDevices.getUserMedia(
        mediaConstraints,
      );

      // Verify stream tracks
      final audioTracks = stream.getAudioTracks();
      final videoTracks = stream.getVideoTracks();

      debugPrint(
        'ğŸ¥ Created stream with ${audioTracks.length} audio tracks and ${videoTracks.length} video tracks',
      );

      // Configure audio tracks for proper earpiece routing
      await _configureAudioTracks(audioTracks, callType);

      // Enable all tracks
      for (final track in audioTracks) {
        track.enabled = true;
        debugPrint('ğŸ¥ Audio track enabled: ${track.enabled}');
      }

      for (final track in videoTracks) {
        track.enabled = true;
        debugPrint('ğŸ¥ Video track enabled: ${track.enabled}');
      }

      localStream = stream;
      localRenderer.srcObject = stream;

      // Small delay to ensure renderer is ready
      await Future.delayed(const Duration(milliseconds: 200));

      return stream;
    } catch (e) {
      debugPrint('ğŸ¥ Error creating media stream: $e');
      rethrow;
    }
  }

  /// Configure audio tracks for proper communication routing
  static Future<void> _configureAudioTracks(
    List<MediaStreamTrack> audioTracks,
    String callType,
  ) async {
    try {
      debugPrint(
        'ğŸ¥ ğŸ”Š Configuring ${audioTracks.length} audio tracks for $callType call',
      );

      for (final track in audioTracks) {
        // Configure audio track constraints for better voice quality and earpiece routing
        await track.applyConstraints({
          'echoCancellation': true,
          'noiseSuppression': false, // Disable to reduce processing noise
          'autoGainControl': false, // Disable to reduce processing noise
          'sampleRate': 16000, // Optimize for voice
          'channelCount': 1, // Mono for voice calls
        });

        debugPrint('ğŸ¥ ğŸ”Š Audio track configured: ${track.id}');
      }
    } catch (e) {
      debugPrint('ğŸ¥ ğŸ”Š Error configuring audio tracks: $e');
      // Don't rethrow - track configuration failure shouldn't prevent stream creation
    }
  }

  /// Create peer connection with proper configuration
  static Future<RTCPeerConnection> _createPeerConnection() async {
    try {
      debugPrint('ğŸ¥ Creating peer connection');

      final pc = await createPeerConnection(_iceServers);

      // Set up event handlers with immediate ICE candidate sending
      pc.onIceCandidate = (candidate) {
        if (peerId != null && candidate.candidate != null) {
          debugPrint('ğŸ¥ ğŸ“¤ Sending ICE candidate immediately');
          // Send candidate immediately for faster connection establishment
          SocketService.socket.emit('webrtc-candidate', {
            'toUserId': peerId!,
            'candidate': {
              'candidate': candidate.candidate,
              'sdpMid': candidate.sdpMid,
              'sdpMLineIndex': candidate.sdpMLineIndex,
            },
          });
        }
      };

      pc.onTrack = (event) {
        debugPrint('ğŸ¥ Received remote track: ${event.track.kind}');

        if (event.streams.isNotEmpty) {
          final remoteStream = event.streams[0];
          debugPrint(
            'ğŸ¥ Setting remote stream with ${remoteStream.getTracks().length} tracks',
          );

          // Enable remote tracks
          for (final track in remoteStream.getTracks()) {
            track.enabled = true;
            debugPrint(
              'ğŸ¥ Remote ${track.kind} track enabled: ${track.enabled}',
            );
          }

          remoteRenderer.srcObject = remoteStream;
        }
      };

      pc.onConnectionState = (state) {
        debugPrint('ğŸ¥ ğŸ“Š Connection state changed: $state');

        if (state == RTCPeerConnectionState.RTCPeerConnectionStateConnected) {
          debugPrint('ğŸ¥ âœ… WebRTC connection established successfully!');

          // Force earpiece mode for voice calls when connection is established
          forceEarpieceMode();

          _onConnectionEstablished?.call();
        } else if (state ==
            RTCPeerConnectionState.RTCPeerConnectionStateConnecting) {
          debugPrint('ğŸ¥ ğŸ”„ WebRTC connecting...');
        } else if (state ==
            RTCPeerConnectionState.RTCPeerConnectionStateFailed) {
          debugPrint('ğŸ¥ âŒ WebRTC connection failed - check network/firewall');
        } else if (state ==
            RTCPeerConnectionState.RTCPeerConnectionStateDisconnected) {
          debugPrint('ğŸ¥ ğŸ”Œ WebRTC disconnected');
        }
      };

      pc.onIceConnectionState = (state) {
        debugPrint('ğŸ¥ ğŸ§Š ICE connection state: $state');

        // Early connection detection - when ICE completes, connection is ready
        if (state == RTCIceConnectionState.RTCIceConnectionStateConnected) {
          debugPrint('ğŸ¥ ğŸ§Š âœ… ICE connected - peer-to-peer link established!');
        } else if (state ==
            RTCIceConnectionState.RTCIceConnectionStateCompleted) {
          debugPrint('ğŸ¥ ğŸ§Š âœ… ICE gathering completed!');
        } else if (state == RTCIceConnectionState.RTCIceConnectionStateFailed) {
          debugPrint('ğŸ¥ ğŸ§Š âŒ ICE connection failed - network issues detected');
        }
      };

      return pc;
    } catch (e) {
      debugPrint('ğŸ¥ Error creating peer connection: $e');
      rethrow;
    }
  }

  /// Start call as offerer
  static Future<void> startCall(String targetUserId, String callType) async {
    try {
      debugPrint('ğŸ¥ Starting $callType call to $targetUserId');

      // Ensure WebRTC is initialized
      if (!isInitialized) {
        debugPrint('ğŸ¥ WebRTC not initialized, initializing now');
        await init();
      }

      peerId = targetUserId;
      isOfferer = true;
      currentCallType = callType;

      // Start audio routing for the call
      final audioCallType = callType == 'video'
          ? CallType.video
          : CallType.voice;
      await AudioRoutingService.instance.startCall(audioCallType);
      debugPrint('ğŸ¥ ğŸ”Š Audio routing started for $callType call');

      // Create media stream
      final stream = await createMediaStream(callType);

      // Create peer connection
      peerConnection = await _createPeerConnection();

      // Add tracks to peer connection
      for (final track in stream.getTracks()) {
        await peerConnection!.addTrack(track, stream);
        debugPrint('ğŸ¥ Added ${track.kind} track to peer connection');
      }

      // First, notify the target user about the incoming call
      debugPrint('ğŸ¥ Sending call-user event to $targetUserId');
      SocketService.socket.emit('call-user', {
        'toUserId': targetUserId,
        'metadata': {'type': callType},
      });

      // Create and send offer with optimized constraints for faster negotiation
      final offerOptions = {
        'offerToReceiveAudio': true,
        'offerToReceiveVideo': callType == 'video',
        'iceRestart': false,
      };

      final offer = await peerConnection!.createOffer(offerOptions);
      await peerConnection!.setLocalDescription(offer);

      debugPrint('ğŸ¥ ğŸ“¤ Sending offer immediately (ICE will trickle)');
      SocketService.socket.emit('webrtc-offer', {
        'toUserId': targetUserId,
        'sdp': {'sdp': offer.sdp, 'type': offer.type},
        'metadata': {'type': callType},
      });
    } catch (e) {
      debugPrint('ğŸ¥ Error starting call: $e');
      rethrow;
    }
  }

  /// Answer incoming call
  static Future<void> answerCall(String callerUserId, String callType) async {
    try {
      debugPrint('ğŸ¥ Answering $callType call from $callerUserId');

      // Ensure WebRTC is initialized
      if (!isInitialized) {
        debugPrint('ğŸ¥ WebRTC not initialized, initializing now');
        await init();
      }

      peerId = callerUserId;
      isOfferer = false;
      currentCallType = callType;

      // Start audio routing for the call
      final audioCallType = callType == 'video'
          ? CallType.video
          : CallType.voice;
      await AudioRoutingService.instance.startCall(audioCallType);
      debugPrint('ğŸ¥ ğŸ”Š Audio routing started for $callType call');

      // Create media stream
      final stream = await createMediaStream(callType);

      // Create peer connection
      peerConnection = await _createPeerConnection();

      // Add tracks to peer connection
      for (final track in stream.getTracks()) {
        await peerConnection!.addTrack(track, stream);
        debugPrint('ğŸ¥ Added ${track.kind} track to peer connection');
      }

      // Process any pending offer that arrived before peer connection was ready
      await processPendingOffer();

      debugPrint(
        'ğŸ¥ âœ… answerCall completed - connection callback should be set',
      );
      debugWebRTCState();
    } catch (e) {
      debugPrint('ğŸ¥ Error answering call: $e');
      rethrow;
    }
  }

  /// Handle incoming WebRTC offer
  static Future<void> handleOffer(Map<String, dynamic> data) async {
    try {
      debugPrint('ğŸ¥ ğŸ“¥ Handling offer from ${data['fromUserId']}');

      if (peerConnection == null) {
        debugPrint('ğŸ¥ â³ No peer connection yet, storing offer for later');
        _pendingOffer = data;
        return;
      }

      _isProcessingOffer = true;

      // Set remote description quickly
      final offer = RTCSessionDescription(
        data['sdp']['sdp'],
        data['sdp']['type'],
      );

      await peerConnection!.setRemoteDescription(offer);
      debugPrint('ğŸ¥ âœ… Remote description (offer) set');

      // Process any buffered ICE candidates now that we have remote description
      if (_pendingIceCandidates.isNotEmpty) {
        debugPrint(
          'ğŸ¥ ğŸ§Š Adding ${_pendingIceCandidates.length} buffered ICE candidates',
        );
        for (final candidate in _pendingIceCandidates) {
          try {
            await peerConnection!.addCandidate(candidate);
          } catch (e) {
            debugPrint('ğŸ¥ âš ï¸ Error adding buffered candidate: $e');
          }
        }
        _pendingIceCandidates.clear();
      }

      // Create answer with optimized constraints
      final answerOptions = {
        'offerToReceiveAudio': true,
        'offerToReceiveVideo': currentCallType == 'video',
      };

      final answer = await peerConnection!.createAnswer(answerOptions);
      await peerConnection!.setLocalDescription(answer);

      debugPrint('ğŸ¥ ğŸ“¤ Sending answer immediately');
      SocketService.socket.emit('webrtc-answer', {
        'toUserId': data['fromUserId'],
        'sdp': {'sdp': answer.sdp, 'type': answer.type},
      });

      _isProcessingOffer = false;
      debugPrint('ğŸ¥ âœ… Offer-Answer exchange completed');
    } catch (e) {
      _isProcessingOffer = false;
      debugPrint('ğŸ¥ âŒ Error handling offer: $e');
    }
  }

  /// Handle incoming WebRTC answer
  static Future<void> handleAnswer(Map<String, dynamic> data) async {
    try {
      debugPrint('ğŸ¥ ğŸ“¥ Handling answer from ${data['fromUserId']}');

      if (peerConnection == null) {
        debugPrint('ğŸ¥ âŒ No peer connection available for answer');
        return;
      }

      final answer = RTCSessionDescription(
        data['sdp']['sdp'],
        data['sdp']['type'],
      );

      await peerConnection!.setRemoteDescription(answer);
      debugPrint('ğŸ¥ âœ… Remote description (answer) set');

      // Process any buffered ICE candidates
      if (_pendingIceCandidates.isNotEmpty) {
        debugPrint(
          'ğŸ¥ ğŸ§Š Adding ${_pendingIceCandidates.length} buffered ICE candidates',
        );
        for (final candidate in _pendingIceCandidates) {
          try {
            await peerConnection!.addCandidate(candidate);
          } catch (e) {
            debugPrint('ğŸ¥ âš ï¸ Error adding buffered candidate: $e');
          }
        }
        _pendingIceCandidates.clear();
      }

      debugPrint('ğŸ¥ âœ… Answer processed - waiting for ICE to connect...');
    } catch (e) {
      debugPrint('ğŸ¥ âŒ Error handling answer: $e');
    }
  }

  /// Handle incoming ICE candidate with buffering
  static Future<void> handleCandidate(Map<String, dynamic> data) async {
    try {
      debugPrint('ğŸ¥ ğŸ§Š Received ICE candidate from ${data['fromUserId']}');

      final candidate = RTCIceCandidate(
        data['candidate']['candidate'],
        data['candidate']['sdpMid'],
        data['candidate']['sdpMLineIndex'],
      );

      if (peerConnection == null) {
        debugPrint('ğŸ¥ â³ No peer connection yet, buffering candidate');
        _pendingIceCandidates.add(candidate);
        return;
      }

      // Check if we're ready to add candidates
      if (peerConnection!.signalingState ==
              RTCSignalingState.RTCSignalingStateStable ||
          !_isProcessingOffer) {
        // We can add the candidate immediately
        try {
          await peerConnection!.addCandidate(candidate);
          debugPrint('ğŸ¥ ğŸ§Š âœ… ICE candidate added immediately');
        } catch (e) {
          debugPrint(
            'ğŸ¥ ğŸ§Š âš ï¸ Failed to add candidate immediately, buffering: $e',
          );
          _pendingIceCandidates.add(candidate);
        }
      } else {
        // Buffer it until we complete offer processing
        debugPrint('ğŸ¥ ğŸ§Š â³ Buffering candidate (processing offer)');
        _pendingIceCandidates.add(candidate);
      }
    } catch (e) {
      debugPrint('ğŸ¥ âŒ Error handling ICE candidate: $e');
    }
  }

  /// Toggle microphone
  static Future<void> toggleMicrophone() async {
    try {
      if (localStream != null) {
        final audioTracks = localStream!.getAudioTracks();
        if (audioTracks.isNotEmpty) {
          final track = audioTracks[0];
          track.enabled = !track.enabled;
          debugPrint('ğŸ¥ Microphone ${track.enabled ? 'enabled' : 'disabled'}');
        }
      }
    } catch (e) {
      debugPrint('ğŸ¥ Error toggling microphone: $e');
    }
  }

  /// Toggle camera
  static Future<void> toggleCamera() async {
    try {
      if (localStream != null) {
        final videoTracks = localStream!.getVideoTracks();
        if (videoTracks.isNotEmpty) {
          final track = videoTracks[0];
          track.enabled = !track.enabled;
          debugPrint('ğŸ¥ Camera ${track.enabled ? 'enabled' : 'disabled'}');
        }
      }
    } catch (e) {
      debugPrint('ğŸ¥ Error toggling camera: $e');
    }
  }

  /// Switch camera (front/back)
  static Future<void> switchCamera() async {
    try {
      if (localStream != null) {
        final videoTracks = localStream!.getVideoTracks();
        if (videoTracks.isNotEmpty) {
          await Helper.switchCamera(videoTracks[0]);
          debugPrint('ğŸ¥ Camera switched');
        }
      }
    } catch (e) {
      debugPrint('ğŸ¥ Error switching camera: $e');
    }
  }

  /// Check if microphone is muted
  static bool isMicrophoneMuted() {
    if (localStream != null) {
      final audioTracks = localStream!.getAudioTracks();
      if (audioTracks.isNotEmpty) {
        return !audioTracks[0].enabled;
      }
    }
    return false;
  }

  /// Check if camera is disabled
  static bool isCameraDisabled() {
    if (localStream != null) {
      final videoTracks = localStream!.getVideoTracks();
      if (videoTracks.isNotEmpty) {
        return !videoTracks[0].enabled;
      }
    }
    return false;
  }

  /// Toggle speaker mode
  static Future<void> toggleSpeaker() async {
    try {
      await AudioRoutingService.instance.toggleSpeaker();
      debugPrint('ğŸ”Š Speaker toggled via AudioRoutingService');
    } catch (e) {
      debugPrint('ğŸ”Š Error toggling speaker: $e');
    }
  }

  /// Check if speaker is enabled
  static bool isSpeakerEnabled() {
    return AudioRoutingService.instance.currentAudioRoute == AudioRoute.speaker;
  }

  /// Get current audio route
  static AudioRoute getCurrentAudioRoute() {
    return AudioRoutingService.instance.currentAudioRoute;
  }

  /// Force earpiece mode (for voice calls)
  static Future<void> forceEarpieceMode() async {
    try {
      // This ensures voice calls start with earpiece, not speaker
      if (currentCallType == 'voice') {
        final audioRouting = AudioRoutingService.instance;
        // Force earpiece by ensuring speaker is not forced on
        if (audioRouting.isSpeakerForced) {
          await audioRouting.toggleSpeaker();
        }
        debugPrint('ğŸ”Š Forced earpiece mode for voice call');
      }
    } catch (e) {
      debugPrint('ğŸ”Š Error forcing earpiece mode: $e');
    }
  }

  /// Process any pending offer that was received before peer connection was ready
  static Future<void> processPendingOffer() async {
    if (_pendingOffer != null) {
      debugPrint(
        'ğŸ¥ Processing pending offer from ${_pendingOffer!['fromUserId']}',
      );
      await handleOffer(_pendingOffer!);
      _pendingOffer = null;
    }
  }

  /// Dispose and cleanup
  static Future<void> dispose() async {
    try {
      debugPrint('ğŸ¥ Disposing WebRTC Service');

      // End audio routing service call
      try {
        await AudioRoutingService.instance.endCall();
        debugPrint('ğŸ¥ ğŸ”Š Audio routing call ended');
      } catch (e) {
        debugPrint('ğŸ¥ ğŸ”Š Error ending audio routing call: $e');
      }

      // Stop local stream tracks
      if (localStream != null) {
        for (final track in localStream!.getTracks()) {
          track.stop();
        }
        localStream = null;
      }

      // Clear renderers
      localRenderer.srcObject = null;
      remoteRenderer.srcObject = null;

      // Close peer connection
      if (peerConnection != null) {
        await peerConnection!.close();
        peerConnection = null;
      }

      // Dispose renderers
      if (isInitialized) {
        try {
          await localRenderer.dispose();
        } catch (e) {
          debugPrint('ğŸ¥ Error disposing local renderer: $e');
        }

        try {
          await remoteRenderer.dispose();
        } catch (e) {
          debugPrint('ğŸ¥ Error disposing remote renderer: $e');
        }
      }

      // Cleanup audio routing and session
      try {
        await AudioRoutingService.instance.endCall();
        debugPrint('ğŸ¥ ğŸ”Š Audio routing cleanup completed');
      } catch (e) {
        debugPrint('ğŸ¥ ğŸ”Š Error cleaning up audio routing: $e');
      }

      try {
        final session = await AudioSession.instance;
        await session.setActive(false);
        debugPrint('ğŸ¥ ğŸ”Š Audio session deactivated');
      } catch (e) {
        debugPrint('ğŸ¥ ğŸ”Š Error deactivating audio session: $e');
      }

      // Reset state
      isInitialized = false;
      isOfferer = false;
      peerId = null;
      currentCallType = 'voice';
      _pendingOffer = null;
      _pendingIceCandidates.clear();
      _isProcessingOffer = false;

      debugPrint('ğŸ¥ WebRTC Service disposed');
    } catch (e) {
      debugPrint('ğŸ¥ Error disposing WebRTC: $e');
    }
  }

  /// Debug audio tracks status
  static void debugAudioStatus() {
    debugPrint('ğŸ¥ === AUDIO DEBUG ===');

    if (localStream != null) {
      final audioTracks = localStream!.getAudioTracks();
      debugPrint('ğŸ¥ Local audio tracks: ${audioTracks.length}');
      for (int i = 0; i < audioTracks.length; i++) {
        final track = audioTracks[i];
        debugPrint(
          'ğŸ¥ Local audio $i: enabled=${track.enabled}, muted=${track.muted}',
        );
      }
    }

    final remoteStream = remoteRenderer.srcObject;
    if (remoteStream != null) {
      final audioTracks = remoteStream.getAudioTracks();
      debugPrint('ğŸ¥ Remote audio tracks: ${audioTracks.length}');
      for (int i = 0; i < audioTracks.length; i++) {
        final track = audioTracks[i];
        debugPrint(
          'ğŸ¥ Remote audio $i: enabled=${track.enabled}, muted=${track.muted}',
        );
      }
    }

    debugPrint('ğŸ¥ === END AUDIO DEBUG ===');
  }
}
